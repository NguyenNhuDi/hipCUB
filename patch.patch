From 5f2fdd7871220e9558c41581a405d15e73941b14 Mon Sep 17 00:00:00 2001
From: monoatamd <mnorouzi@amd.com>
Date: Thu, 1 Feb 2024 15:44:23 +0000
Subject: [PATCH] Add missing hipCUB APIs

---
 hipcub/include/hipcub/hipcub_ext.cuh          |  86 +++
 .../hipcub/single_pass_scan_operators.cuh     | 582 +++++++++++++++
 hipcub/include/hipcub/thread_load.cuh         |  29 +
 hipcub/include/hipcub/util_device.cuh         | 702 ++++++++++++++++++
 4 files changed, 1399 insertions(+)
 create mode 100644 hipcub/include/hipcub/hipcub_ext.cuh
 create mode 100644 hipcub/include/hipcub/single_pass_scan_operators.cuh
 create mode 100644 hipcub/include/hipcub/thread_load.cuh
 create mode 100644 hipcub/include/hipcub/util_device.cuh

diff --git a/hipcub/include/hipcub/hipcub_ext.cuh b/hipcub/include/hipcub/hipcub_ext.cuh
new file mode 100644
index 0000000..a1d78ca
--- /dev/null
+++ b/hipcub/include/hipcub/hipcub_ext.cuh
@@ -0,0 +1,86 @@
+#ifndef HIBCUB_EXT
+#define HIBCUB_EXT
+#include "hip/hip_runtime.h"
+#include "hipcub.hpp"
+
+#ifndef HIPCUB_QUOTIENT_CEILING
+    /// Quotient of x/y rounded up to nearest integer
+    #define HIPCUB_QUOTIENT_CEILING(x, y) (((x) + (y) - 1) / (y))
+#endif
+
+#if defined(__HIP_DEVICE_COMPILE__)
+    #define HIPCUB_IS_DEVICE_CODE 1
+    #define HIPCUB_IS_HOST_CODE 0
+    #define HIPCUB_INCLUDE_DEVICE_CODE 1
+    #define HIPCUB_INCLUDE_HOST_CODE 0
+#else
+    #define HIPCUB_IS_DEVICE_CODE 0
+    #define HIPCUB_IS_HOST_CODE 1
+    #define HIPCUB_INCLUDE_DEVICE_CODE 0
+    #define HIPCUB_INCLUDE_HOST_CODE 1
+#endif
+/// Maximum number of devices supported.
+#ifndef HIPCUB_MAX_DEVICES
+    #define HIPCUB_MAX_DEVICES 128
+#endif
+
+#if HIPCUB_CPP_DIALECT >= 2011
+    static_assert(HIPCUB_MAX_DEVICES > 0, "HIPCUB_MAX_DEVICES must be greater than 0.");
+#endif
+
+/// Whether or not the source targeted by the active compiler pass is allowed to  invoke device kernels or methods from the CUDA runtime API.
+#if !defined(__HIP_DEVICE_COMPILE__)
+    #define HIPCUB_RUNTIME_ENABLED
+#endif
+
+namespace hipcub_extensions {
+
+namespace detail
+{
+template <bool Test, class T1, class T2>
+using conditional_t = typename std::conditional<Test, T1, T2>::type;
+
+/**
+ * Call `cudaDeviceSynchronize()` using the proper API for the current CUB and
+ * CUDA configuration.
+ */
+// HIPCUB_EXEC_CHECK_DISABLE
+HIPCUB_RUNTIME_FUNCTION inline hipError_t device_synchronize()
+{
+  hipError_t result = hipErrorUnknown;
+
+  if (HIPCUB_IS_HOST_CODE)
+  {
+#if HIPCUB_INCLUDE_HOST_CODE
+    result = hipDeviceSynchronize();
+#endif
+  }
+  else
+  {
+    // Device code with the CUDA runtime.
+#if defined(HIPCUB_INCLUDE_DEVICE_CODE) && defined(HIPCUB_RUNTIME_ENABLED)
+
+#if defined(__HIPCC__) &&                                                     \
+  ((__HIPCC_VER_MAJOR__ > 11) ||                                              \
+   ((__HIPCC_VER_MAJOR__ == 11) && (__HIPCC_VER_MINOR__ >= 6)))
+    // CUDA >= 11.6
+    result = __hipDeviceSynchronizeDeprecationAvoidance();
+#else // CUDA < 11.6
+    result = hipDeviceSynchronize();
+#endif
+
+#else // Device code without the CUDA runtime.
+    // Device side CUDA API calls are not supported in this configuration.
+    result = hipErrorInvalidConfiguration;
+#endif
+  }
+
+  return result;
+}
+}
+
+#include "util_device.cuh"
+#include "single_pass_scan_operators.cuh"
+}// End of namespace hipcub_extensions
+
+#endif
\ No newline at end of file
diff --git a/hipcub/include/hipcub/single_pass_scan_operators.cuh b/hipcub/include/hipcub/single_pass_scan_operators.cuh
new file mode 100644
index 0000000..b0b6868
--- /dev/null
+++ b/hipcub/include/hipcub/single_pass_scan_operators.cuh
@@ -0,0 +1,582 @@
+/******************************************************************************
+ * Copyright (c) 2011, Duane Merrill.  All rights reserved.
+ * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ ******************************************************************************/
+
+/**
+ * \file
+ * Callback operator types for supplying BlockScan prefixes
+ */
+
+#pragma once
+
+#include <iterator>
+
+#include "thread_load.cuh"
+
+// TODO Support NVIDIA arch + AMD warpsize 32
+using lane_mask = uint64_t;
+constexpr lane_mask LANE_MASK_ALL = ~lane_mask{0};
+
+/******************************************************************************
+ * Prefix functor type for maintaining a running prefix while scanning a
+ * region independent of other thread blocks
+ ******************************************************************************/
+
+/**
+ * Stateful callback operator type for supplying BlockScan prefixes.
+ * Maintains a running prefix that can be applied to consecutive
+ * BlockScan operations.
+ */
+template <
+    typename T,                 ///< BlockScan value type
+    typename ScanOpT>            ///< Wrapped scan operator type
+struct BlockScanRunningPrefixOp
+{
+    ScanOpT     op;                 ///< Wrapped scan operator
+    T           running_total;      ///< Running block-wide prefix
+
+    /// Constructor
+    __device__ __forceinline__ BlockScanRunningPrefixOp(ScanOpT op)
+    :
+        op(op)
+    {}
+
+    /// Constructor
+    __device__ __forceinline__ BlockScanRunningPrefixOp(
+        T starting_prefix,
+        ScanOpT op)
+    :
+        op(op),
+        running_total(starting_prefix)
+    {}
+
+    /**
+     * Prefix callback operator.  Returns the block-wide running_total in thread-0.
+     */
+    __device__ __forceinline__ T operator()(
+        const T &block_aggregate)              ///< The aggregate sum of the BlockScan inputs
+    {
+        T retval = running_total;
+        running_total = op(running_total, block_aggregate);
+        return retval;
+    }
+};
+
+
+/******************************************************************************
+ * Generic tile status interface types for block-cooperative scans
+ ******************************************************************************/
+
+/**
+ * Enumerations of tile status
+ */
+enum ScanTileStatus
+{
+    SCAN_TILE_OOB,          // Out-of-bounds (e.g., padding)
+    SCAN_TILE_INVALID = 99, // Not yet processed
+    SCAN_TILE_PARTIAL,      // Tile aggregate is available
+    SCAN_TILE_INCLUSIVE,    // Inclusive tile prefix is available
+};
+
+
+/**
+ * Tile status interface.
+ */
+template <
+    typename    T,
+    bool        SINGLE_WORD = hipcub::Traits<T>::PRIMITIVE>
+struct ScanTileState;
+
+
+/**
+ * Tile status interface specialized for scan status and value types
+ * that can be combined into one machine word that can be
+ * read/written coherently in a single access.
+ */
+template <typename T>
+struct ScanTileState<T, true>
+{
+    // Status word type
+    using StatusWord = hipcub_extensions::detail::conditional_t<
+      sizeof(T) == 8,
+      long long,
+      hipcub_extensions::detail::conditional_t<
+        sizeof(T) == 4,
+        int,
+        hipcub_extensions::detail::conditional_t<sizeof(T) == 2, short, char>>>;
+
+    // Unit word type
+    using TxnWord = hipcub_extensions::detail::conditional_t<
+      sizeof(T) == 8,
+      longlong2,
+      hipcub_extensions::detail::conditional_t<
+        sizeof(T) == 4,
+        int2,
+        hipcub_extensions::detail::conditional_t<sizeof(T) == 2, int, uchar2>>>;
+
+    // Device word type
+    struct TileDescriptor
+    {
+        StatusWord  status;
+        T           value;
+    };
+
+
+    // Constants
+    enum
+    {
+        TILE_STATUS_PADDING = HIPCUB_WARP_THREADS,
+    };
+
+
+    // Device storage
+    TxnWord *d_tile_descriptors;
+
+    /// Constructor
+    __host__ __device__ __forceinline__
+    ScanTileState()
+    :
+        d_tile_descriptors(NULL)
+    {}
+
+
+    /// Initializer
+    __host__ __device__ __forceinline__
+    hipError_t Init(
+        int     /*num_tiles*/,                      ///< [in] Number of tiles
+        void    *d_temp_storage,                    ///< [in] Device-accessible allocation of temporary storage.  When NULL, the required allocation size is written to \p temp_storage_bytes and no work is done.
+        size_t  /*temp_storage_bytes*/)             ///< [in] Size in bytes of \t d_temp_storage allocation
+    {
+        d_tile_descriptors = reinterpret_cast<TxnWord*>(d_temp_storage);
+        return hipSuccess;
+    }
+
+
+    /**
+     * Compute device memory needed for tile status
+     */
+    __host__ __device__ __forceinline__
+    static hipError_t AllocationSize(
+        int     num_tiles,                          ///< [in] Number of tiles
+        size_t  &temp_storage_bytes)                ///< [out] Size in bytes of \t d_temp_storage allocation
+    {
+        temp_storage_bytes = (num_tiles + TILE_STATUS_PADDING) * sizeof(TileDescriptor);       // bytes needed for tile status descriptors
+        return hipSuccess;
+    }
+
+
+    /**
+     * Initialize (from device)
+     */
+    __device__ __forceinline__ void InitializeStatus(int num_tiles)
+    {
+        int tile_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
+
+        TxnWord val = TxnWord();
+        TileDescriptor *descriptor = reinterpret_cast<TileDescriptor*>(&val);
+
+        if (tile_idx < num_tiles)
+        {
+            // Not-yet-set
+            descriptor->status = StatusWord(SCAN_TILE_INVALID);
+            d_tile_descriptors[TILE_STATUS_PADDING + tile_idx] = val;
+        }
+
+        if ((blockIdx.x == 0) && (threadIdx.x < TILE_STATUS_PADDING))
+        {
+            // Padding
+            descriptor->status = StatusWord(SCAN_TILE_OOB);
+            d_tile_descriptors[threadIdx.x] = val;
+        }
+    }
+
+
+    /**
+     * Update the specified tile's inclusive value and corresponding status
+     */
+    __device__ __forceinline__ void SetInclusive(int tile_idx, T tile_inclusive)
+    {
+        TileDescriptor tile_descriptor;
+        tile_descriptor.status = SCAN_TILE_INCLUSIVE;
+        tile_descriptor.value = tile_inclusive;
+
+        TxnWord alias;
+        *reinterpret_cast<TileDescriptor*>(&alias) = tile_descriptor;
+        hipcub::ThreadStore<hipcub::STORE_CG>(d_tile_descriptors + TILE_STATUS_PADDING + tile_idx, alias);
+    }
+
+
+    /**
+     * Update the specified tile's partial value and corresponding status
+     */
+    __device__ __forceinline__ void SetPartial(int tile_idx, T tile_partial)
+    {
+        TileDescriptor tile_descriptor;
+        tile_descriptor.status = SCAN_TILE_PARTIAL;
+        tile_descriptor.value = tile_partial;
+
+        TxnWord alias;
+        *reinterpret_cast<TileDescriptor*>(&alias) = tile_descriptor;
+        hipcub::ThreadStore<hipcub::STORE_CG>(d_tile_descriptors + TILE_STATUS_PADDING + tile_idx, alias);
+    }
+
+    /**
+     * Wait for the corresponding tile to become non-invalid
+     */
+    __device__ __forceinline__ void WaitForValid(
+        int             tile_idx,
+        StatusWord      &status,
+        T               &value)
+    {
+        TileDescriptor tile_descriptor;
+        do
+        {
+            // Todo(HIP): We changed it from threadfence_block to threadfence 
+            // because it created a runtime issue: FST_TEST hangs in TEST_F(FstTest, GroundTruth).
+            // Finite-State Transducer (FST) is a key component of nested JSON parser. 
+            // Parallel FST algorithm is described here: https://eliasstehle.com/media/parparaw_vldb_2020.pdf
+            // It looks highly that blocks of threads cooperatively work together to parse a json input.
+            // Hipcub APIs are used to collect info from all threads in all blocks.
+            // If the result produced by one thread in a block is not seen by another thread in a different block,
+            // then the computed hipcub operation is not correct and threads may result in a busy waiting state.
+            // Might be related to https://github.com/AMD-AI/hipdf/issues/71
+            __threadfence(); // prevent hoisting loads from loop
+            TxnWord alias = hipcub_extensions::ThreadLoad<hipcub::LOAD_CG>(d_tile_descriptors + TILE_STATUS_PADDING + tile_idx);
+            tile_descriptor = reinterpret_cast<TileDescriptor&>(alias);
+
+        } while (hipcub::WARP_ANY((tile_descriptor.status == SCAN_TILE_INVALID), LANE_MASK_ALL));
+
+        status = tile_descriptor.status;
+        value = tile_descriptor.value;
+    }
+
+};
+
+
+
+/**
+ * Tile status interface specialized for scan status and value types that
+ * cannot be combined into one machine word.
+ */
+template <typename T>
+struct ScanTileState<T, false>
+{
+    // Status word type
+    typedef char StatusWord;
+
+    // Constants
+    enum
+    {
+        TILE_STATUS_PADDING = HIPCUB_WARP_THREADS,
+    };
+
+    // Device storage
+    StatusWord  *d_tile_status;
+    T           *d_tile_partial;
+    T           *d_tile_inclusive;
+
+    /// Constructor
+    __host__ __device__ __forceinline__
+    ScanTileState()
+    :
+        d_tile_status(NULL),
+        d_tile_partial(NULL),
+        d_tile_inclusive(NULL)
+    {}
+
+
+    /// Initializer
+    __host__ __device__ __forceinline__
+    hipError_t Init(
+        int     num_tiles,                          ///< [in] Number of tiles
+        void    *d_temp_storage,                    ///< [in] Device-accessible allocation of temporary storage.  When NULL, the required allocation size is written to \p temp_storage_bytes and no work is done.
+        size_t  temp_storage_bytes)                 ///< [in] Size in bytes of \t d_temp_storage allocation
+    {
+        hipError_t error = hipSuccess;
+        do
+        {
+            void*   allocations[3] = {};
+            size_t  allocation_sizes[3];
+
+            allocation_sizes[0] = (num_tiles + TILE_STATUS_PADDING) * sizeof(StatusWord);           // bytes needed for tile status descriptors
+            allocation_sizes[1] = (num_tiles + TILE_STATUS_PADDING) * sizeof(hipcub::Uninitialized<T>);     // bytes needed for partials
+            allocation_sizes[2] = (num_tiles + TILE_STATUS_PADDING) * sizeof(hipcub::Uninitialized<T>);     // bytes needed for inclusives
+
+            // Compute allocation pointers into the single storage blob
+            if (HipcubDebug(error = AliasTemporaries(d_temp_storage, temp_storage_bytes, allocations, allocation_sizes))) break;
+
+            // Alias the offsets
+            d_tile_status       = reinterpret_cast<StatusWord*>(allocations[0]);
+            d_tile_partial      = reinterpret_cast<T*>(allocations[1]);
+            d_tile_inclusive    = reinterpret_cast<T*>(allocations[2]);
+        }
+        while (0);
+
+        return error;
+    }
+
+
+    /**
+     * Compute device memory needed for tile status
+     */
+    __host__ __device__ __forceinline__
+    static hipError_t AllocationSize(
+        int     num_tiles,                          ///< [in] Number of tiles
+        size_t  &temp_storage_bytes)                ///< [out] Size in bytes of \t d_temp_storage allocation
+    {
+        // Specify storage allocation requirements
+        size_t  allocation_sizes[3];
+        allocation_sizes[0] = (num_tiles + TILE_STATUS_PADDING) * sizeof(StatusWord);         // bytes needed for tile status descriptors
+        allocation_sizes[1] = (num_tiles + TILE_STATUS_PADDING) * sizeof(hipcub::Uninitialized<T>);   // bytes needed for partials
+        allocation_sizes[2] = (num_tiles + TILE_STATUS_PADDING) * sizeof(hipcub::Uninitialized<T>);   // bytes needed for inclusives
+
+        // Set the necessary size of the blob
+        void* allocations[3] = {};
+        return HipcubDebug(AliasTemporaries(NULL, temp_storage_bytes, allocations, allocation_sizes));
+    }
+
+
+    /**
+     * Initialize (from device)
+     */
+    __device__ __forceinline__ void InitializeStatus(int num_tiles)
+    {
+        int tile_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
+        if (tile_idx < num_tiles)
+        {
+            // Not-yet-set
+            d_tile_status[TILE_STATUS_PADDING + tile_idx] = StatusWord(SCAN_TILE_INVALID);
+        }
+
+        if ((blockIdx.x == 0) && (threadIdx.x < TILE_STATUS_PADDING))
+        {
+            // Padding
+            d_tile_status[threadIdx.x] = StatusWord(SCAN_TILE_OOB);
+        }
+    }
+
+
+    /**
+     * Update the specified tile's inclusive value and corresponding status
+     */
+    __device__ __forceinline__ void SetInclusive(int tile_idx, T tile_inclusive)
+    {
+        // Update tile inclusive value
+        hipcub::ThreadStore<hipcub::STORE_CG>(d_tile_inclusive + TILE_STATUS_PADDING + tile_idx, tile_inclusive);
+
+        // Fence
+        __threadfence();
+
+        // Update tile status
+        hipcub::ThreadStore<hipcub::STORE_CG>(d_tile_status + TILE_STATUS_PADDING + tile_idx, StatusWord(SCAN_TILE_INCLUSIVE));
+    }
+
+
+    /**
+     * Update the specified tile's partial value and corresponding status
+     */
+    __device__ __forceinline__ void SetPartial(int tile_idx, T tile_partial)
+    {
+        // Update tile partial value
+        hipcub::ThreadStore<hipcub::STORE_CG>(d_tile_partial + TILE_STATUS_PADDING + tile_idx, tile_partial);
+
+        // Fence
+        __threadfence();
+
+        // Update tile status
+        hipcub::ThreadStore<hipcub::STORE_CG>(d_tile_status + TILE_STATUS_PADDING + tile_idx, StatusWord(SCAN_TILE_PARTIAL));
+    }
+
+    /**
+     * Wait for the corresponding tile to become non-invalid
+     */
+    __device__ __forceinline__ void WaitForValid(
+        int             tile_idx,
+        StatusWord      &status,
+        T               &value)
+    {
+        do {
+            status = hipcub_extensions::ThreadLoad<hipcub::LOAD_CG>(d_tile_status + TILE_STATUS_PADDING + tile_idx);
+
+            __threadfence();    // prevent hoisting loads from loop or loads below above this one
+
+        } while (status == SCAN_TILE_INVALID);
+
+        if (status == StatusWord(SCAN_TILE_PARTIAL)) 
+            value = hipcub_extensions::ThreadLoad<hipcub::LOAD_CG>(d_tile_partial + TILE_STATUS_PADDING + tile_idx);
+        else
+            value = hipcub_extensions::ThreadLoad<hipcub::LOAD_CG>(d_tile_inclusive + TILE_STATUS_PADDING + tile_idx);
+    }
+};
+
+
+/******************************************************************************
+ * Prefix call-back operator for coupling local block scan within a
+ * block-cooperative scan
+ ******************************************************************************/
+
+/**
+ * Stateful block-scan prefix functor.  Provides the the running prefix for
+ * the current tile by using the call-back warp to wait on on
+ * aggregates/prefixes from predecessor tiles to become available.
+ */
+template <
+    typename    T,
+    typename    ScanOpT,
+    typename    ScanTileStateT,
+    int         PTX_ARCH = HIPCUB_ARCH>
+struct TilePrefixCallbackOp
+{
+    // Parameterized warp reduce
+    typedef hipcub::WarpReduce<T, HIPCUB_WARP_THREADS, PTX_ARCH> WarpReduceT;
+
+    // Temporary storage type
+    struct _TempStorage
+    {
+        typename WarpReduceT::TempStorage   warp_reduce;
+        T                                   exclusive_prefix;
+        T                                   inclusive_prefix;
+        T                                   block_aggregate;
+    };
+
+    // Alias wrapper allowing temporary storage to be unioned
+    struct TempStorage : hipcub::Uninitialized<_TempStorage> {};
+
+    // Type of status word
+    typedef typename ScanTileStateT::StatusWord StatusWord;
+
+    // Fields
+    _TempStorage&               temp_storage;       ///< Reference to a warp-reduction instance
+    ScanTileStateT&             tile_status;        ///< Interface to tile status
+    ScanOpT                     scan_op;            ///< Binary scan operator
+    int                         tile_idx;           ///< The current tile index
+    T                           exclusive_prefix;   ///< Exclusive prefix for the tile
+    T                           inclusive_prefix;   ///< Inclusive prefix for the tile
+
+    // Constructor
+    __device__ __forceinline__
+    TilePrefixCallbackOp(
+        ScanTileStateT       &tile_status,
+        TempStorage         &temp_storage,
+        ScanOpT              scan_op,
+        int                 tile_idx)
+    :
+        temp_storage(temp_storage.Alias()),
+        tile_status(tile_status),
+        scan_op(scan_op),
+        tile_idx(tile_idx) {}
+
+
+    // Block until all predecessors within the warp-wide window have non-invalid status
+    __device__ __forceinline__
+    void ProcessWindow(
+        int         predecessor_idx,        ///< Preceding tile index to inspect
+        StatusWord  &predecessor_status,    ///< [out] Preceding tile status
+        T           &window_aggregate)      ///< [out] Relevant partial reduction from this window of preceding tiles
+    {
+        T value;
+        tile_status.WaitForValid(predecessor_idx, predecessor_status, value);
+
+        // Perform a segmented reduction to get the prefix for the current window.
+        // Use the swizzled scan operator because we are now scanning *down* towards thread0.
+
+        int tail_flag = (predecessor_status == StatusWord(SCAN_TILE_INCLUSIVE));
+        window_aggregate = WarpReduceT(temp_storage.warp_reduce).TailSegmentedReduce(
+            value,
+            tail_flag,
+            hipcub::SwizzleScanOp<ScanOpT>(scan_op));
+    }
+
+
+    // BlockScan prefix callback functor (called by the first warp)
+    __device__ __forceinline__
+    T operator()(T block_aggregate)
+    {
+
+        // Update our status with our tile-aggregate
+        if (threadIdx.x == 0)
+        {
+            temp_storage.block_aggregate = block_aggregate;
+            tile_status.SetPartial(tile_idx, block_aggregate);
+        }
+
+        int         predecessor_idx = tile_idx - threadIdx.x - 1;
+        StatusWord  predecessor_status;
+        T           window_aggregate;
+
+        // Wait for the warp-wide window of predecessor tiles to become valid
+        ProcessWindow(predecessor_idx, predecessor_status, window_aggregate);
+
+        // The exclusive tile prefix starts out as the current window aggregate
+        exclusive_prefix = window_aggregate;
+
+        // Keep sliding the window back until we come across a tile whose inclusive prefix is known
+        while (hipcub::WARP_ALL((predecessor_status != StatusWord(SCAN_TILE_INCLUSIVE)), LANE_MASK_ALL))
+        {
+            predecessor_idx -= HIPCUB_WARP_THREADS;
+
+            // Update exclusive tile prefix with the window prefix
+            ProcessWindow(predecessor_idx, predecessor_status, window_aggregate);
+            exclusive_prefix = scan_op(window_aggregate, exclusive_prefix);
+        }
+
+        // Compute the inclusive tile prefix and update the status for this tile
+        if (threadIdx.x == 0)
+        {
+            inclusive_prefix = scan_op(exclusive_prefix, block_aggregate);
+            tile_status.SetInclusive(tile_idx, inclusive_prefix);
+
+            temp_storage.exclusive_prefix = exclusive_prefix;
+            temp_storage.inclusive_prefix = inclusive_prefix;
+        }
+
+        // Return exclusive_prefix
+        return exclusive_prefix;
+    }
+
+    // Get the exclusive prefix stored in temporary storage
+    __device__ __forceinline__
+    T GetExclusivePrefix()
+    {
+        return temp_storage.exclusive_prefix;
+    }
+
+    // Get the inclusive prefix stored in temporary storage
+    __device__ __forceinline__
+    T GetInclusivePrefix()
+    {
+        return temp_storage.inclusive_prefix;
+    }
+
+    // Get the block aggregate stored in temporary storage
+    __device__ __forceinline__
+    T GetBlockAggregate()
+    {
+        return temp_storage.block_aggregate;
+    }
+
+};
\ No newline at end of file
diff --git a/hipcub/include/hipcub/thread_load.cuh b/hipcub/include/hipcub/thread_load.cuh
new file mode 100644
index 0000000..bf3249e
--- /dev/null
+++ b/hipcub/include/hipcub/thread_load.cuh
@@ -0,0 +1,29 @@
+#ifndef THREADLOAD
+#define THREADLOAD
+
+template<hipcub::CacheLoadModifier MODIFIER = hipcub::LOAD_DEFAULT, typename T>
+HIPCUB_DEVICE __forceinline__ T AsmThreadLoad(void * ptr)
+{
+    T retval; // We removed the intialization to 0 because some of the data types cannot be initialized to 0.
+    // Also, retval is set to ptr with the builtin
+    __builtin_memcpy(&retval, ptr, sizeof(T));
+    return retval;
+}
+
+template<hipcub::CacheLoadModifier MODIFIER = hipcub::LOAD_DEFAULT, typename InputIteratorT>
+HIPCUB_DEVICE __forceinline__
+typename std::iterator_traits<InputIteratorT>::value_type ThreadLoad(InputIteratorT itr)
+{
+    using T  = typename std::iterator_traits<InputIteratorT>::value_type;
+    T retval = ThreadLoad<MODIFIER>(&(*itr));
+    return retval;
+}
+
+template<hipcub::CacheLoadModifier MODIFIER = hipcub::LOAD_DEFAULT, typename T>
+HIPCUB_DEVICE __forceinline__ T
+ThreadLoad(T * ptr)
+{
+    return AsmThreadLoad<MODIFIER, T>(ptr);
+}
+
+#endif
\ No newline at end of file
diff --git a/hipcub/include/hipcub/util_device.cuh b/hipcub/include/hipcub/util_device.cuh
new file mode 100644
index 0000000..c7a3623
--- /dev/null
+++ b/hipcub/include/hipcub/util_device.cuh
@@ -0,0 +1,702 @@
+#include "hip/hip_runtime.h"
+/******************************************************************************
+ * Copyright (c) 2011, Duane Merrill.  All rights reserved.
+ * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ ******************************************************************************/
+
+/**
+ * \file
+ * Properties of a given CUDA device and the corresponding PTX bundle
+ */
+
+#pragma once
+
+#include <atomic>
+#include <array>
+#include <cassert>
+
+/**
+ * \addtogroup UtilMgmt
+ * @{
+ */
+
+#ifndef DOXYGEN_SHOULD_SKIP_THIS // Do not document
+
+
+/**
+ * \brief Alias temporaries to externally-allocated device storage (or simply return the amount of storage needed).
+ */
+template <int ALLOCATIONS>
+__host__ __device__ __forceinline__
+hipError_t AliasTemporaries(
+    void    *d_temp_storage,                    ///< [in] Device-accessible allocation of temporary storage.  When NULL, the required allocation size is written to \p temp_storage_bytes and no work is done.
+    size_t& temp_storage_bytes,                 ///< [in,out] Size in bytes of \t d_temp_storage allocation
+    void*   (&allocations)[ALLOCATIONS],        ///< [in,out] Pointers to device allocations needed
+    size_t  (&allocation_sizes)[ALLOCATIONS])   ///< [in] Sizes in bytes of device allocations needed
+{
+    const int ALIGN_BYTES   = 256;
+    const int ALIGN_MASK    = ~(ALIGN_BYTES - 1);
+
+    // Compute exclusive prefix sum over allocation requests
+    size_t allocation_offsets[ALLOCATIONS];
+    size_t bytes_needed = 0;
+    for (int i = 0; i < ALLOCATIONS; ++i)
+    {
+        size_t allocation_bytes = (allocation_sizes[i] + ALIGN_BYTES - 1) & ALIGN_MASK;
+        allocation_offsets[i] = bytes_needed;
+        bytes_needed += allocation_bytes;
+    }
+    bytes_needed += ALIGN_BYTES - 1;
+
+    // Check if the caller is simply requesting the size of the storage allocation
+    if (!d_temp_storage)
+    {
+        temp_storage_bytes = bytes_needed;
+        return hipSuccess;
+    }
+
+    // Check if enough storage provided
+    if (temp_storage_bytes < bytes_needed)
+    {
+        return HipcubDebug(hipErrorInvalidValue);
+    }
+
+    // Alias
+    d_temp_storage = (void *) ((size_t(d_temp_storage) + ALIGN_BYTES - 1) & ALIGN_MASK);
+    for (int i = 0; i < ALLOCATIONS; ++i)
+    {
+        allocations[i] = static_cast<char*>(d_temp_storage) + allocation_offsets[i];
+    }
+
+    return hipSuccess;
+}
+
+
+/**
+ * \brief Empty kernel for querying PTX manifest metadata (e.g., version) for the current device
+ */
+template <typename T>
+__global__ void EmptyKernel(void) { }
+
+#endif  // DOXYGEN_SHOULD_SKIP_THIS
+
+/**
+ * \brief Returns the current device or -1 if an error occurred.
+ */
+HIPCUB_RUNTIME_FUNCTION inline int CurrentDevice()
+{
+#if defined(HIPCUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
+
+    int device = -1;
+    if (HipcubDebug(hipGetDevice(&device))) return -1;
+    return device;
+
+#else // Device code without the CUDA runtime.
+
+    return -1;
+
+#endif
+}
+
+/**
+ * \brief RAII helper which saves the current device and switches to the
+ *        specified device on construction and switches to the saved device on
+ *        destruction.
+ */
+struct SwitchDevice
+{
+private:
+    int const old_device;
+    bool const needs_reset;
+public:
+    __host__ inline SwitchDevice(int new_device)
+      : old_device(CurrentDevice()), needs_reset(old_device != new_device)
+    {
+        if (needs_reset)
+            auto a = hipSetDevice(new_device);
+            // HipcubDebug();
+    }
+
+    __host__ inline ~SwitchDevice()
+    {
+        if (needs_reset)
+            auto a = hipSetDevice(old_device);
+            // HipcubDebug();
+    }
+};
+
+/**
+ * \brief Returns the number of CUDA devices available or -1 if an error
+ *        occurred.
+ */
+HIPCUB_RUNTIME_FUNCTION inline int DeviceCountUncached()
+{
+#if defined(HIPCUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
+
+    int count = -1;
+    if (HipcubDebug(hipGetDeviceCount(&count)))
+        // CUDA makes no guarantees about the state of the output parameter if
+        // `hipGetDeviceCount` fails; in practice, they don't, but out of
+        // paranoia we'll reset `count` to `-1`.
+        count = -1;
+    return count;
+
+#else // Device code without the CUDA runtime.
+
+    return -1;
+
+#endif
+}
+
+#if HIPCUB_CPP_DIALECT >= 2011 // C++11 and later.
+
+/**
+ * \brief Cache for an arbitrary value produced by a nullary function.
+ */
+template <typename T, T(*Function)()>
+struct ValueCache
+{
+    T const value;
+
+    /**
+     * \brief Call the nullary function to produce the value and construct the
+     *        cache.
+     */
+    __host__ inline ValueCache() : value(Function()) {}
+};
+
+#endif
+
+#if HIPCUB_CPP_DIALECT >= 2011
+// Host code, only safely usable in C++11 or newer, where thread-safe
+// initialization of static locals is guaranteed.  This is a separate function
+// to avoid defining a local static in a host/device function.
+__host__ inline int DeviceCountCachedValue()
+{
+    static ValueCache<int, DeviceCountUncached> cache;
+    return cache.value;
+}
+#endif
+
+/**
+ * \brief Returns the number of CUDA devices available.
+ *
+ * \note This function may cache the result internally.
+ *
+ * \note This function is thread safe.
+ */
+HIPCUB_RUNTIME_FUNCTION inline int DeviceCount()
+{
+    int result = -1;
+    if (HIPCUB_IS_HOST_CODE) {
+        #if HIPCUB_INCLUDE_HOST_CODE
+            #if HIPCUB_CPP_DIALECT >= 2011
+                // Host code and C++11.
+                result = DeviceCountCachedValue();
+            #else
+                // Host code and C++98.
+                result = DeviceCountUncached();
+            #endif
+        #endif
+    } else {
+        #if HIPCUB_INCLUDE_DEVICE_CODE
+            // Device code.
+            result = DeviceCountUncached();
+        #endif
+    }
+    return result;
+}
+
+#if HIPCUB_CPP_DIALECT >= 2011 // C++11 and later.
+
+/**
+ * \brief Per-device cache for a CUDA attribute value; the attribute is queried
+ *        and stored for each device upon construction.
+ */
+struct PerDeviceAttributeCache
+{
+    struct DevicePayload
+    {
+        int         attribute;
+        hipError_t error;
+    };
+
+    // Each entry starts in the `DeviceEntryEmpty` state, then proceeds to the
+    // `DeviceEntryInitializing` state, and then proceeds to the
+    // `DeviceEntryReady` state. These are the only state transitions allowed;
+    // e.g. a linear sequence of transitions.
+    enum DeviceEntryStatus
+    {
+        DeviceEntryEmpty = 0,
+        DeviceEntryInitializing,
+        DeviceEntryReady
+    };
+
+    struct DeviceEntry
+    {
+        std::atomic<DeviceEntryStatus> flag;
+        DevicePayload                  payload;
+    };
+
+private:
+    std::array<DeviceEntry, HIPCUB_MAX_DEVICES> entries_;
+
+public:
+    /**
+     * \brief Construct the cache.
+     */
+    __host__ inline PerDeviceAttributeCache() : entries_()
+    {
+        assert(DeviceCount() <= HIPCUB_MAX_DEVICES);
+    }
+
+    /**
+     * \brief Retrieves the payload of the cached function \p f for \p device.
+     *
+     * \note You must pass a morally equivalent function in to every call or
+     *       this function has undefined behavior.
+     */
+    template <typename Invocable>
+    __host__ DevicePayload operator()(Invocable&& f, int device)
+    {
+        if (device >= DeviceCount())
+            return DevicePayload{0, hipErrorInvalidDevice};
+
+        auto& entry   = entries_[device];
+        auto& flag    = entry.flag;
+        auto& payload = entry.payload;
+
+        DeviceEntryStatus old_status = DeviceEntryEmpty;
+
+        // First, check for the common case of the entry being ready.
+        if (flag.load(std::memory_order_acquire) != DeviceEntryReady)
+        {
+            // Assume the entry is empty and attempt to lock it so we can fill
+            // it by trying to set the state from `DeviceEntryReady` to
+            // `DeviceEntryInitializing`.
+            if (flag.compare_exchange_strong(old_status, DeviceEntryInitializing,
+                                             std::memory_order_acq_rel,
+                                             std::memory_order_acquire))
+            {
+                // We successfully set the state to `DeviceEntryInitializing`;
+                // we have the lock and it's our job to initialize this entry
+                // and then release it.
+
+                // We don't use `HipcubDebug` here because we let the user code
+                // decide whether or not errors are hard errors.
+                payload.error = std::forward<Invocable>(f)(payload.attribute);
+                if (payload.error)
+                    // Clear the global CUDA error state which may have been
+                    // set by the last call. Otherwise, errors may "leak" to
+                    // unrelated kernel launches.
+                    hipGetLastError();
+
+                // Release the lock by setting the state to `DeviceEntryReady`.
+                flag.store(DeviceEntryReady, std::memory_order_release);
+            }
+
+            // If the `compare_exchange_weak` failed, then `old_status` has
+            // been updated with the value of `flag` that it observed.
+
+            else if (old_status == DeviceEntryInitializing)
+            {
+                // Another execution agent is initializing this entry; we need
+                // to wait for them to finish; we'll know they're done when we
+                // observe the entry status as `DeviceEntryReady`.
+                do { old_status = flag.load(std::memory_order_acquire); }
+                while (old_status != DeviceEntryReady);
+                // FIXME: Use `atomic::wait` instead when we have access to
+                // host-side C++20 atomics. We could use libcu++, but it only
+                // supports atomics for SM60 and up, even if you're only using
+                // them in host code.
+            }
+        }
+
+        // We now know that the state of our entry is `DeviceEntryReady`, so
+        // just return the entry's payload.
+        return entry.payload;
+    }
+};
+
+#endif
+
+/**
+ * \brief Retrieves the PTX version that will be used on the current device (major * 100 + minor * 10).
+ */
+HIPCUB_RUNTIME_FUNCTION inline hipError_t PtxVersionUncached(int& ptx_version)
+{
+    // Instantiate `EmptyKernel<void>` in both host and device code to ensure
+    // it can be called.
+    typedef void (*EmptyKernelPtr)();
+    EmptyKernelPtr empty_kernel = EmptyKernel<void>;
+
+    // This is necessary for unused variable warnings in host compilers. The
+    // usual syntax of (void)empty_kernel; was not sufficient on MSVC2015.
+    (void)reinterpret_cast<void*>(empty_kernel);
+
+    hipError_t result = hipSuccess;
+    if (HIPCUB_IS_HOST_CODE) {
+       #if HIPCUB_INCLUDE_HOST_CODE
+            hipFuncAttributes empty_kernel_attrs;
+
+            result = hipFuncGetAttributes(&empty_kernel_attrs,
+                                           reinterpret_cast<void*>(empty_kernel));
+            CUDF_CUDA_TRY(HipcubDebug(result));
+
+            ptx_version = empty_kernel_attrs.ptxVersion * 10;
+        #endif
+    } else {
+        #if HIPCUB_INCLUDE_DEVICE_CODE
+            // This is necessary to ensure instantiation of EmptyKernel in device code.
+            // The `reinterpret_cast` is necessary to suppress a set-but-unused warnings.
+            // This is a meme now: https://twitter.com/blelbach/status/1222391615576100864
+            (void)reinterpret_cast<EmptyKernelPtr>(empty_kernel);
+
+            ptx_version = HIPCUB_ARCH;
+        #endif
+    }
+    return result;
+}
+
+/**
+ * \brief Retrieves the PTX version that will be used on \p device (major * 100 + minor * 10).
+ */
+__host__ inline hipError_t PtxVersionUncached(int& ptx_version, int device)
+{
+    SwitchDevice sd(device);
+    (void)sd;
+    return PtxVersionUncached(ptx_version);
+}
+
+#if HIPCUB_CPP_DIALECT >= 2011 // C++11 and later.
+template <typename Tag>
+__host__ inline PerDeviceAttributeCache& GetPerDeviceAttributeCache()
+{
+    // C++11 guarantees that initialization of static locals is thread safe.
+    static PerDeviceAttributeCache cache;
+    return cache;
+}
+
+struct PtxVersionCacheTag {};
+struct SmVersionCacheTag {};
+#endif
+
+/**
+ * \brief Retrieves the PTX version that will be used on \p device (major * 100 + minor * 10).
+ *
+ * \note This function may cache the result internally.
+ *
+ * \note This function is thread safe.
+ */
+__host__ inline hipError_t PtxVersion(int& ptx_version, int device)
+{
+#if HIPCUB_CPP_DIALECT >= 2011 // C++11 and later.
+
+    auto const payload = GetPerDeviceAttributeCache<PtxVersionCacheTag>()(
+      // If this call fails, then we get the error code back in the payload,
+      // which we check with `HipcubDebug` below.
+      [=] (int& pv) { return PtxVersionUncached(pv, device); },
+      device);
+
+    if (!HipcubDebug(payload.error))
+        ptx_version = payload.attribute;
+
+    return payload.error;
+
+#else // Pre C++11.
+
+    return PtxVersionUncached(ptx_version, device);
+
+#endif
+}
+
+/**
+ * \brief Retrieves the PTX version that will be used on the current device (major * 100 + minor * 10).
+ *
+ * \note This function may cache the result internally.
+ *
+ * \note This function is thread safe.
+ */
+HIPCUB_RUNTIME_FUNCTION inline hipError_t PtxVersion(int& ptx_version)
+{
+    hipError_t result = hipErrorUnknown;
+    if (HIPCUB_IS_HOST_CODE) {
+        #if HIPCUB_INCLUDE_HOST_CODE
+            #if HIPCUB_CPP_DIALECT >= 2011
+                // Host code and C++11.
+                auto const device = CurrentDevice();
+
+                auto const payload = GetPerDeviceAttributeCache<PtxVersionCacheTag>()(
+                  // If this call fails, then we get the error code back in the payload,
+                  // which we check with `HipcubDebug` below.
+                  [=] (int& pv) { return PtxVersionUncached(pv, device); },
+                  device);
+
+                if (!HipcubDebug(payload.error))
+                    ptx_version = payload.attribute;
+
+                result = payload.error;
+            #else
+                // Host code and C++98.
+                result = PtxVersionUncached(ptx_version);
+            #endif
+        #endif
+    } else {
+        #if HIPCUB_INCLUDE_DEVICE_CODE
+            // Device code.
+            result = PtxVersionUncached(ptx_version);
+        #endif
+    }
+    return result;
+}
+
+/**
+ * \brief Retrieves the SM version of \p device (major * 100 + minor * 10)
+ */
+HIPCUB_RUNTIME_FUNCTION inline hipError_t SmVersionUncached(int& sm_version, int device = CurrentDevice())
+{
+#if defined(HIPCUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
+
+    hipError_t error = hipSuccess;
+    do
+    {
+        int major = 0, minor = 0;
+        if (HipcubDebug(error = hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, device))) break;
+        if (HipcubDebug(error = hipDeviceGetAttribute(&minor, hipDeviceAttributeComputeCapabilityMinor, device))) break;
+        sm_version = major * 100 + minor * 10;
+    }
+    while (0);
+
+    return error;
+
+#else // Device code without the CUDA runtime.
+
+    (void)sm_version;
+    (void)device;
+
+    // CUDA API calls are not supported from this device.
+    return HipcubDebug(hipErrorInvalidConfiguration);
+
+#endif
+}
+
+/**
+ * \brief Retrieves the SM version of \p device (major * 100 + minor * 10)
+ *
+ * \note This function may cache the result internally.
+ *
+ * \note This function is thread safe.
+ */
+HIPCUB_RUNTIME_FUNCTION inline hipError_t SmVersion(int& sm_version, int device = CurrentDevice())
+{
+    hipError_t result = hipErrorUnknown;
+    if (HIPCUB_IS_HOST_CODE) {
+        #if HIPCUB_INCLUDE_HOST_CODE
+            #if HIPCUB_CPP_DIALECT >= 2011
+                // Host code and C++11
+                auto const payload = GetPerDeviceAttributeCache<SmVersionCacheTag>()(
+                  // If this call fails, then we get the error code back in the payload,
+                  // which we check with `HipcubDebug` below.
+                  [=] (int& pv) { return SmVersionUncached(pv, device); },
+                  device);
+
+                if (!HipcubDebug(payload.error))
+                    sm_version = payload.attribute;
+
+                result = payload.error;
+            #else
+                // Host code and C++98
+                result = SmVersionUncached(sm_version, device);
+            #endif
+        #endif
+    } else {
+        #if HIPCUB_INCLUDE_DEVICE_CODE
+            result = SmVersionUncached(sm_version, device);
+        #endif
+    }
+    return result;
+}
+
+/**
+ * Synchronize the specified \p stream.
+ */
+HIPCUB_RUNTIME_FUNCTION inline hipError_t SyncStream(hipStream_t stream)
+{
+    hipError_t result = hipErrorUnknown;
+    if (HIPCUB_IS_HOST_CODE) {
+        #if HIPCUB_INCLUDE_HOST_CODE
+            result = HipcubDebug(hipStreamSynchronize(stream));
+        #endif
+    } else {
+        #if HIPCUB_INCLUDE_DEVICE_CODE
+            #if defined(HIPCUB_RUNTIME_ENABLED) // Device code with the CUDA runtime.
+                (void)stream;
+                // Device can't yet sync on a specific stream
+                result = HipcubDebug(hipcub_extensions::detail::device_synchronize());
+            #else // Device code without the CUDA runtime.
+                (void)stream;
+                // CUDA API calls are not supported from this device.
+                result = HipcubDebug(hipErrorInvalidConfiguration);
+            #endif
+        #endif
+    }
+    return result;
+}
+
+
+/**
+ * \brief Computes maximum SM occupancy in thread blocks for executing the given kernel function pointer \p kernel_ptr on the current device with \p block_threads per thread block.
+ *
+ * \par Snippet
+ * The code snippet below illustrates the use of the MaxSmOccupancy function.
+ * \par
+ * \code
+ * #include <hipcub/hipcub.hpp>   // or equivalently <cub/util_device.cuh>
+ *
+ * template <typename T>
+ * __global__ void ExampleKernel()
+ * {
+ *     // Allocate shared memory for BlockScan
+ *     __shared__ volatile T buffer[4096];
+ *
+ *        ...
+ * }
+ *
+ *     ...
+ *
+ * // Determine SM occupancy for ExampleKernel specialized for unsigned char
+ * int max_sm_occupancy;
+ * MaxSmOccupancy(max_sm_occupancy, ExampleKernel<unsigned char>, 64);
+ *
+ * // max_sm_occupancy  <-- 4 on SM10
+ * // max_sm_occupancy  <-- 8 on SM20
+ * // max_sm_occupancy  <-- 12 on SM35
+ *
+ * \endcode
+ *
+ */
+template <typename KernelPtr>
+HIPCUB_RUNTIME_FUNCTION inline
+hipError_t MaxSmOccupancy(
+    int&                max_sm_occupancy,          ///< [out] maximum number of thread blocks that can reside on a single SM
+    KernelPtr           kernel_ptr,                 ///< [in] Kernel pointer for which to compute SM occupancy
+    int                 block_threads,              ///< [in] Number of threads per thread block
+    int                 dynamic_smem_bytes = 0)
+{
+#ifndef HIPCUB_RUNTIME_ENABLED
+
+    (void)dynamic_smem_bytes;
+    (void)block_threads;
+    (void)kernel_ptr;
+    (void)max_sm_occupancy;
+
+    // CUDA API calls not supported from this device
+    return HipcubDebug(hipErrorInvalidConfiguration);
+
+#else
+
+    return HipcubDebug(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+        &max_sm_occupancy,
+        kernel_ptr,
+        block_threads,
+        dynamic_smem_bytes));
+
+#endif  // HIPCUB_RUNTIME_ENABLED
+}
+
+
+/******************************************************************************
+ * Policy management
+ ******************************************************************************/
+
+/**
+ * Kernel dispatch configuration
+ */
+struct KernelConfig
+{
+    int block_threads;
+    int items_per_thread;
+    int tile_size;
+    int sm_occupancy;
+
+    HIPCUB_RUNTIME_FUNCTION __forceinline__
+    KernelConfig() : block_threads(0), items_per_thread(0), tile_size(0), sm_occupancy(0) {}
+
+    template <typename AgentPolicyT, typename KernelPtrT>
+    HIPCUB_RUNTIME_FUNCTION __forceinline__
+    hipError_t Init(KernelPtrT kernel_ptr)
+    {
+        block_threads        = AgentPolicyT::BLOCK_THREADS;
+        items_per_thread     = AgentPolicyT::ITEMS_PER_THREAD;
+        tile_size            = block_threads * items_per_thread;
+        hipError_t retval   = MaxSmOccupancy(sm_occupancy, kernel_ptr, block_threads);
+        return retval;
+    }
+};
+
+
+
+/// Helper for dispatching into a policy chain
+template <int PTX_VERSION, typename PolicyT, typename PrevPolicyT>
+struct ChainedPolicy
+{
+  /// The policy for the active compiler pass
+  using ActivePolicy = 
+    hipcub_extensions::detail::conditional_t<(HIPCUB_ARCH < PTX_VERSION),
+                               typename PrevPolicyT::ActivePolicy,
+                               PolicyT>;
+
+  /// Specializes and dispatches op in accordance to the first policy in the chain of adequate PTX version
+  template <typename FunctorT>
+  HIPCUB_RUNTIME_FUNCTION __forceinline__
+  static hipError_t Invoke(int ptx_version, FunctorT& op)
+  {
+      if (ptx_version < PTX_VERSION) {
+          return PrevPolicyT::Invoke(ptx_version, op);
+      }
+      return op.template Invoke<PolicyT>();
+  }
+};
+
+/// Helper for dispatching into a policy chain (end-of-chain specialization)
+template <int PTX_VERSION, typename PolicyT>
+struct ChainedPolicy<PTX_VERSION, PolicyT, PolicyT>
+{
+    /// The policy for the active compiler pass
+    typedef PolicyT ActivePolicy;
+
+    /// Specializes and dispatches op in accordance to the first policy in the chain of adequate PTX version
+    template <typename FunctorT>
+    HIPCUB_RUNTIME_FUNCTION __forceinline__
+    static hipError_t Invoke(int /*ptx_version*/, FunctorT& op) {
+        return op.template Invoke<PolicyT>();
+    }
+};
+
+
+
+
+/** @} */       // end group UtilMgmt
\ No newline at end of file
-- 
2.34.1


